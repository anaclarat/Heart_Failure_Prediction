{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ab7c71",
   "metadata": {},
   "source": [
    "# Heart Failure Prediction\n",
    "\n",
    "Kaggle excerpt:\n",
    "\n",
    ">Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\n",
    "    Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n",
    "\n",
    ">Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n",
    "\n",
    ">People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n",
    "\n",
    "\n",
    "Data Source: https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data\n",
    "\n",
    "Davide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020). Available at: https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269aaf72",
   "metadata": {},
   "source": [
    "## Dataset original features:\n",
    "    - age: Age\n",
    "    - anaemia: Decrease of red blood cells or hemoglobin (boolean - 0 = No, 1 = Yes)\n",
    "    - creatinine_phosphokinase: Level of the CPK enzyme in the blood (mcg/L) [Used to investigate if there's tissues stress or damage)\n",
    "    - diabetes: If the patient has diabetes (boolean - 0 = No, 1 = Yes)\n",
    "    - ejection_fraction: Percentage of blood leaving the heart at each contraction (percentage)\n",
    "    - high_blood_pressure: If the patient has hypertension (boolean - 0 = No, 1 = Yes)\n",
    "    - platelets: Platelets in the blood (kiloplatelets/mL)\n",
    "    - serum_creatinine: Level of serum creatinine in the blood (mg/dL) [waste product that comes from the normal wear and tear on muscles of the body]\n",
    "    - serum_sodium: Level of serum sodium in the blood (mEq/L) [helps determine electrolite balancee renal function]\n",
    "    - sex: Woman or man (binary - Male = 1, Female =0)\n",
    "    - smoking: 0 = No, 1 = Yes\n",
    "    - time: captures the time of the event. That is, the time at which the patient died or were censored.\n",
    "    - DEATH_EVENT: encodes whether the patient died (1) or whether they were censored (0). Censoring means that the scientists lost contact with the patient.\n",
    "        \n",
    "Features descriptions gathered from kaggle \"About this dataset\" and \"Discussion\" sections. Additional information between '[]'.        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b5811",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import researchpy as rp\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import levene\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbdf2ff",
   "metadata": {},
   "source": [
    "## User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef20bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chck_miss(df):\n",
    "    '''Checking dataset for missing values distribution'''\n",
    "\n",
    "    Total = df.isnull().count().sort_values(ascending=True)        \n",
    "    Missing = df.isnull().sum().sort_values(ascending=True)   \n",
    "\n",
    "    Percentage_Miss = round((Missing/Total)*100,2)\n",
    "\n",
    "    Summary = pd.concat([Total,Missing, Percentage_Miss],axis=1,keys=['Total','# Missing','% Missing'],sort=True)\n",
    "          \n",
    "    print('Are there any missing value?')                    \n",
    "    print(Summary)         \n",
    "\n",
    "    del Total, Missing, Percentage_Miss, Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14aef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_level(df):\n",
    "    '''Checking dataset categorical features levels'''\n",
    "    print(\"Checking the categories levels of categorical features: \\n\")\n",
    "    for i in df.columns:\n",
    "        if (df[i].dtype == object):\n",
    "            print('These are the categories of \\'',i,' \\':\\n',df[i].unique(), '\\n')\n",
    "    del i \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa09516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df,maintain='last'):\n",
    "    '''Checking duplicate values in dataset'''\n",
    "    \n",
    "    res = df.duplicated().sum()\n",
    "    print('There are', res , 'duplicated values in dataset.' )\n",
    "    if res != 0 :\n",
    "        df.drop_duplicates(keep = maintain,inplace=True)\n",
    "        if maintain == 'last':\n",
    "            print('Duplicated values were dropped maintaning only the last occurrence')\n",
    "        else:\n",
    "             print('Duplicated values were dropped maintaning only the first occurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc52360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_numerical(df,visual='Y',test='N',just_summr='N',alpha=0.05):\n",
    "    '''This function returns summarization and visual for univariate analysis of numerical features of a dataset. \n",
    "    Parameters: dataset,test(Shapiro('S'),Anderson-Darling('AD'), Kolmogorov-Smirnov('KS'), all('A') or None('N'), \n",
    "    plot visuals('Y' or 'N',default='Y'),significance level(alpha, default=0.05(0.15,0.1,0.05,0.025,0.01))), \n",
    "    just_summr if set to 'Y' will present just the normality tests assumptions (no other parameter will be \n",
    "    considered) and summary table with normality tests results for the data set('Y' or 'N',default='N')''' \n",
    "\n",
    "#Include The Anderson-Darling test for normality and table with summary an statistical and p-values for normality tests\n",
    "\n",
    "    print('\\nExploratory Data Analysis')\n",
    "    print('\\n############### Univariate analysis - Numerical ###############\\n')\n",
    "    print('\\n\\nFor normality tests of datasets with few variables, better perform a visual check,e.g: Q-Q plot.')\n",
    "    print('\\n\\nFor large datasets, you can present just a summary of normality tests and and other measures values by setting parameter just_summr=\"y\".\\n\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "# Importing packages\n",
    "    from statsmodels.graphics.gofplots import qqplot\n",
    "    from seaborn_qqplot import pplot\n",
    "    import stemgraphic\n",
    "    #https://github.com/fdion/stemgraphic/blob/master/doc/stemgraphic%20A%20Stem-and-Leaf%20Plot%20for%20the%20Age%20of%20Big%20Data.pdf\n",
    "    \n",
    "    from scipy.stats import norm\n",
    "    from scipy.stats import kstest\n",
    "    from scipy.stats import shapiro\n",
    "    from scipy.stats import anderson\n",
    "    \n",
    "    \n",
    "# Setting parameters\n",
    "    test = test    \n",
    "    alpha = alpha\n",
    "    visual = visual\n",
    "    just_summr = just_summr\n",
    "    \n",
    "    \n",
    "    #For AD normality test - pvalues\n",
    "    switcher = {0.15:0,\n",
    "                0.1:1,\n",
    "                0.05:2,\n",
    "                0.025:3,\n",
    "                0.01:4}\n",
    "    \n",
    "# Defining functions\n",
    "\n",
    "    #stem-and-leaf plot\n",
    "    def sl_plot(x,i):\n",
    "        fig,ax = stemgraphic.graphic.stem_graphic(x,title='Stem-and_leaf plot for '+ str(i),asc=True,flip_axes=False,\n",
    "                                                  mirror =False, legend_pos=None,delimiter_color='w',alpha=.5,\n",
    "                                                  font_kw={'size':4},median_color='r',median_alpha=0.7,\n",
    "                                                  primary_kw={'size_inches':(10, 15)},figure_only=True )\n",
    "        #stemgraphic.stem_hist(x, color='b',delimiter_color='w',legend_pos=None)\n",
    "        #fig, ax = stemgraphic.stem_graphic(x,flip_axes=True)\n",
    "        #plt.title(\"Stem-and-Leaf plot for \",i)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #Normality tests\n",
    "    def Kolmogorov(x,i):\n",
    "        # The test statistic developed by Kolmogorov and Smirnov \n",
    "        # to compare distributions was simply the maximum vertical distance between the two functions\n",
    "        x=x\n",
    "        i=i\n",
    "        \n",
    "        print('Kolmogorov-Smirnov normality test\\n')\n",
    "        print('Mostly used with large samples (n > 50)') ####################### TO confirm!!!!!!!!!!\n",
    "        print('Assumptions One-sample KS test:\\n')\n",
    "        print('1. The sample is a random sample')\n",
    "        print('2. The theoretical distribution must be fully specified. The critical values given in tables ')\n",
    "        print('   (and often by software packages) assume this to be the case. If parameters are estimated from the ')\n",
    "        print('   data, the test result will be (much) too conservative. If parameters are estimated from the sample, ')\n",
    "        print('   Lilliefors test should be used instead.) ')\n",
    "        print('3. The theoretical distribution is assumed to be continuous. If it is discrete (for example the Poisson),')\n",
    "        print('   the result will be too conservative, although Conover (1999) provides an equivalent approach for ')\n",
    "        print('   discrete distributions for small samples.[Check Stem-and-leaf plot]')\n",
    "        print('4. The sample distribution is assumed to have no ties. If there are ties (for example from rounding, ')\n",
    "        print('   or if the variable under consideration is discrete), the result will be (much) too liberal as the ')\n",
    "        print('   large steps give an excessively large d. A categorized distribution can be tested with ')\n",
    "        print('   Kolmogorov-Smirnov by dividing observed differences between cumulative distributions by the number of ')\n",
    "        print('   observations in the class interval (n). But such a test is too conservative given (a) the distribution')\n",
    "        print('   is discrete (see above) and (b) power is reduced because the number of observations reduced by a ')\n",
    "        print('   factor of n. \\n')\n",
    "        print('Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693611/')\n",
    "        print('\\n')\n",
    "        \n",
    "        stat, p_normal = kstest(x,'norm') \n",
    "        \n",
    "        print('Kolmogorov-Smirnov\\'s Statistics=%.3f, p=%.3f' % (stat, p_normal))\n",
    "        \n",
    "        return p_normal\n",
    "\n",
    "    \n",
    "    def Shapiro(x):\n",
    "        # The Shapiro test is based on the correlation between the data and the corresponding normal scores \n",
    "        \n",
    "        print(\"\\nShapiro's test\\n\")\n",
    "        print('The Shapiro test is based on the correlation between the data and the corresponding normal scores.')\n",
    "        print('As the dataset being evaluated gets larger, the Shapiro-Wilk test tends to be slightly more ')\n",
    "        print(    'sensitive to data in the tails which leads to a greater probability of rejecting the null hypothesis ')\n",
    "        print('Source: https://influentialpoints.com/Training/kolmogorov-smirnov_test-principles-properties-assumptions.htm')\n",
    "        print('\\n')\n",
    "        \n",
    "        stat, p_normal = shapiro(x) \n",
    "        \n",
    "        print('Shapiro\\'s Statistics=%.3f, p=%.3f' % (stat, p_normal))\n",
    "        \n",
    "        \n",
    "        \n",
    "        return p_normal\n",
    "    \n",
    "    def AnderDarlin(x):\n",
    "        #Documentation:https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html\n",
    "                \n",
    "        print(\"\\nAnderson-Darling's test\\n\")\n",
    "        print('The Anderson-Darling test is severely affected by ties[check Stem-and-Leaf plot] in the data due to poor precision.') \n",
    "        print(    'When a significant number of ties exist, the Anderson-Darling will frequently reject the data ')\n",
    "        print(    'as non-normal, regardless of how well the data fits the normal distribution.')\n",
    "        print('If the returned statistic is larger than these critical values then for the corresponding significance level,')\n",
    "        print(    'the null hypothesis that the data come from the chosen distribution can be rejected. ')\n",
    "        print('Source: https://variation.com/wp-content/distribution_analyzer_help/hs140.htm')\n",
    "        print('\\n')\n",
    "    \n",
    "        stat, crit_values, sig = anderson(x,dist='norm') \n",
    "        \n",
    "        #switcher = {0.15:0,\n",
    "        #    0.1:1,\n",
    "        #    0.05:2,\n",
    "        #    0.025:3,\n",
    "         #   0.01:4}\n",
    "\n",
    "        def switch(alpha1) :\n",
    "            alpha1=alpha1\n",
    "            return switcher.get(alpha1)\n",
    "        \n",
    "        idx=switch(alpha)\n",
    "        print('Anderson-Darling\\'s Statistics=%.3f, critical value=%.3f' % (stat, crit_values[idx]))\n",
    "        \n",
    "        print('\\nHypothesis:')\n",
    "        print('\\nH0: Sample may be Gaussian')\n",
    "        print('H1: Sample does not looks Gaussian ')\n",
    "        \n",
    "        print('\\nSignificance level:',alpha)\n",
    "        \n",
    "        if stat > crit_values[idx]:\n",
    "            print('\\nNormality\\'s test result: Sample does not look Gaussian (reject H0)\\n\\n')\n",
    "        else:\n",
    "            print('\\nNormality\\'s test result: Sample looks Gaussian (fail to reject H0)\\n\\n')\n",
    "   \n",
    "    \n",
    "    def normal_test(x,i):\n",
    "        #x =  feature, i = feature's name\n",
    "        '''Normality test for a feature'''\n",
    "    # normality test\n",
    "        print('\\n############### Normality tests ###############\\n')\n",
    "        i = i\n",
    "        x =  x  \n",
    "        #print(\"Normality test for: \",i,\"\\n\")\n",
    "        #stat, p_normal = shapiro(df[i])\n",
    "        if test == 'KS':\n",
    "            sl_plot(x,i)\n",
    "            p_normal = Kolmogorov(x,i) \n",
    "            # interpretation\n",
    "            interp(p_normal)\n",
    "        elif test == 'S':\n",
    "            p_normal = Shapiro(x)\n",
    "            # interpretation\n",
    "            interp(p_normal)\n",
    "        elif test == 'AD':\n",
    "            sl_plot(x,i)\n",
    "            p_normal = AnderDarlin(x)\n",
    "               \n",
    "        else:\n",
    "            #test\n",
    "            sl_plot(x,i)\n",
    "            p_normal = Kolmogorov(x,i) \n",
    "            # interpretation\n",
    "            interp(p_normal)\n",
    "            print('___________________________\\n\\n')   \n",
    "            #test\n",
    "            p_normal = AnderDarlin(x)\n",
    "            print('___________________________\\n\\n') \n",
    "            #test\n",
    "            p_normal = Shapiro(x)\n",
    "            # interpretation\n",
    "            interp(p_normal)\n",
    "\n",
    "                \n",
    "        print('_____________________________________________________________________________\\n\\n')\n",
    "        \n",
    "        \n",
    "    def interp(p_norm):\n",
    "        '''This function presents interpretation for normality test p-value'''\n",
    "        p_normal = p_norm\n",
    "        # interpretation\n",
    "        print('\\nHypothesis:')\n",
    "        print('\\nH0: Sample may be Gaussian')\n",
    "        print('H1: Sample does not looks Gaussian ')\n",
    "        \n",
    "        print('\\nSignificance level:',alpha)\n",
    "        \n",
    "        if p_normal > alpha:\n",
    "            print('\\nNormality\\'s test result: Sample looks Gaussian (fail to reject H0)\\n\\n')\n",
    "        else:\n",
    "            print('\\nNormality\\'s test result: Sample does not look Gaussian (reject H0)\\n\\n')\n",
    "        \n",
    "    def visual_plot(x,i):\n",
    "        #x =  feature, i = feature's name\n",
    "        '''This function presents visuals for Uninumerical EDA'''\n",
    "        features = x\n",
    "        i = i\n",
    "    # Visualization\n",
    "        # Histogram and Boxplot\n",
    "        f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.12, .88)})\n",
    "        f.set_size_inches(10, 6.5)\n",
    "    #   sns.distplot(a=df[i], hist=True, kde=True, rug=True, ax=ax_hist)\n",
    "        sns.histplot(x=x, kde=True, ax=ax_hist)\n",
    "        sns.boxplot(x=x, ax=ax_box)\n",
    "        ax_box.set_title('Univariate plot')\n",
    "        ax_box.set(xlabel='')\n",
    "        f.suptitle(\"Plots for \"+ str(i))\n",
    "        plt.show()\n",
    "        \n",
    "        #Q-Q plot (against a normal distribution)\n",
    "         \n",
    "        pplot(df, x=i, y=norm, kind='qq',height=8,display_kws={\"identity\":True})\n",
    "        plt.title(\"QQ Plot\")\n",
    "        plt.show()\n",
    "        print('_____________________________________________________________________________\\n\\n')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def sumr(x):\n",
    "        '''This function presents summary for Uninumerical EDA'''\n",
    "    # Setting canvas  \n",
    "        sns.color_palette(\"dark\")    #, as_cmap=True\n",
    "        sns.set_context(\"talk\", font_scale=1) #paper, notebook, talk, poster\n",
    "    # Summarization   \n",
    "        table = pd.DataFrame() #table to summarise tests statitics and measures\n",
    "        \n",
    "        KolmogorovStat=[]\n",
    "        KolmogorovPval=[]\n",
    "        \n",
    "        ADStat=[]\n",
    "        ADCrit=[]\n",
    "        \n",
    "        ShapiroStat=[]\n",
    "        ShapiroPval=[]\n",
    "        \n",
    "        \n",
    "        df.describe()\n",
    "        Summary = dict(df.describe())\n",
    "        features = []\n",
    "        for i in Summary:\n",
    "            Summary[i]['count'] = df[i].count()\n",
    "            Summary[i]['variance'] = df[i].var()\n",
    "            Summary[i]['IQR'] = Summary[i]['75%'] - Summary[i]['25%']\n",
    "            Summary[i]['range'] = Summary[i]['max'] - Summary[i]['min']\n",
    "            Summary[i]['skewness'] = df[i].skew()\n",
    "            Summary[i]['kurtosis'] = df[i].kurtosis()\n",
    "            Summary[i]['mode'] = df[i].mode().values #axis='columns'\n",
    "            #print('\\n This is univariate analysis for',\"'\", i, \"'\", '\\n\\n')\n",
    "           # print('############### Summary ###############')\n",
    "            #display(pd.DataFrame(Summary[i]))\n",
    "            \n",
    "\n",
    "            #KS values\n",
    "            features.append(i)\n",
    "            table = table.append(Summary[i], ignore_index=True)\n",
    "            stat, p_normal = kstest(df[i],'norm') \n",
    "            KolmogorovStat.append(stat)\n",
    "            KolmogorovPval.append(p_normal)\n",
    "                \n",
    "            #AD values\n",
    "            stat, crit_values, sig = anderson(df[i],dist='norm') \n",
    "        \n",
    "            def switch(alpha1) :\n",
    "                alpha1=alpha1\n",
    "                return switcher.get(alpha1)\n",
    "        \n",
    "            idx=switch(alpha)\n",
    "            stat, crit_values = stat, crit_values[idx]\n",
    "        \n",
    "            ADStat.append(stat)\n",
    "            ADCrit.append(crit_values)\n",
    "                \n",
    "                \n",
    "            #S values\n",
    "            stat, p_normal = shapiro(df[i])\n",
    "            ShapiroStat.append(stat)\n",
    "            ShapiroPval.append(p_normal)\n",
    "            \n",
    "            if just_summr != 'Y':   \n",
    "                print('\\n This is univariate analysis for',\"'\", i, \"'\", '\\n\\n')\n",
    "                print('############### Summary ###############')\n",
    "                display(pd.DataFrame(Summary[i]))\n",
    "            \n",
    "                if test != 'N' and visual == 'N':\n",
    "                    normal_test(df[i],i) \n",
    "                elif test != 'N' and visual == 'Y':\n",
    "                    normal_test(df[i],i)                 \n",
    "                    visual_plot(df[i],i)\n",
    "                elif test == 'N' and visual == 'Y':    \n",
    "                    visual_plot(df[i],i)\n",
    "\n",
    "        table = pd.concat([table,pd.DataFrame(features,columns=['features'])],axis=1, ignore_index=False)\n",
    "        table['Defined Significance Level'] = alpha\n",
    "        \n",
    "        table = pd.concat([table,pd.DataFrame(KolmogorovStat,columns=['Kolmogorov Statistics'])],axis=1, ignore_index=False)\n",
    "        table = pd.concat([table,pd.DataFrame(KolmogorovPval,columns=['Kolmogorov P-value'])],axis=1, ignore_index=False)\n",
    "        \n",
    "        table = pd.concat([table,pd.DataFrame(ADStat,columns=['Anderson-Darling Statistics'])],axis=1, ignore_index=False)\n",
    "        table = pd.concat([table,pd.DataFrame(ADCrit,columns=['Anderson-Darling Critical Value'])],axis=1, ignore_index=False)\n",
    "        \n",
    "        table = pd.concat([table,pd.DataFrame(ShapiroStat,columns=['Shapiro Statistics'])],axis=1, ignore_index=False)\n",
    "        table = pd.concat([table,pd.DataFrame(ShapiroPval,columns=['Shapiro P-value'])],axis=1, ignore_index=False)\n",
    "         \n",
    "        if just_summr != 'Y':   \n",
    "            print('\\n This is univariate analysis for',\"'\", i, \"'\", '\\n\\n')\n",
    "            print('############### Summary ###############')\n",
    "            display(pd.DataFrame(Summary[i]))\n",
    "            \n",
    "            if test != 'N' and visual == 'N':\n",
    "                normal_test(df[i],i) \n",
    "            elif test != 'N' and visual == 'Y':\n",
    "                normal_test(df[i],i)                 \n",
    "                visual_plot(df[i],i)\n",
    "            elif test == 'N' and visual == 'Y':    \n",
    "                visual_plot(df[i],i)\n",
    "              \n",
    "        \n",
    "        print('############### Summary ###############\\n')    \n",
    "        \n",
    "        print('\\nHypothesis:')\n",
    "        print('\\nH0: Sample may be Gaussian')\n",
    "        print('H1: Sample does not looks Gaussian \\n ')\n",
    "        \n",
    "        print('Kolmogorov-Smirnov normality test\\n')\n",
    "        print('Mostly used with large samples (n > 50)') ####################### TO confirm!!!!!!!!!!\n",
    "        print('Assumptions One-sample KS test:\\n')\n",
    "        print('1. The sample is a random sample')\n",
    "        print('2. The theoretical distribution must be fully specified. The critical values given in tables ')\n",
    "        print('   (and often by software packages) assume this to be the case. If parameters are estimated from the ')\n",
    "        print('   data, the test result will be (much) too conservative. If parameters are estimated from the sample, ')\n",
    "        print('   Lilliefors test should be used instead.) ')\n",
    "        print('3. The theoretical distribution is assumed to be continuous. If it is discrete (for example the Poisson),')\n",
    "        print('   the result will be too conservative, although Conover (1999) provides an equivalent approach for ')\n",
    "        print('   discrete distributions for small samples.[Check Stem-and-leaf plot]')\n",
    "        print('4. The sample distribution is assumed to have no ties. If there are ties (for example from rounding, ')\n",
    "        print('   or if the variable under consideration is discrete), the result will be (much) too liberal as the ')\n",
    "        print('   large steps give an excessively large d. A categorized distribution can be tested with ')\n",
    "        print('   Kolmogorov-Smirnov by dividing observed differences between cumulative distributions by the number of ')\n",
    "        print('   observations in the class interval (n). But such a test is too conservative given (a) the distribution')\n",
    "        print('   is discrete (see above) and (b) power is reduced because the number of observations reduced by a ')\n",
    "        print('   factor of n. \\n')\n",
    "        print('Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693611/')\n",
    "        print('\\n')        \n",
    "        \n",
    "        print(\"\\nAnderson-Darling's test\\n\")\n",
    "        print('The Anderson-Darling test is severely affected by ties[check Stem-and-Leaf plot] in the data due to poor precision.') \n",
    "        print(    'When a significant number of ties exist, the Anderson-Darling will frequently reject the data ')\n",
    "        print(    'as non-normal, regardless of how well the data fits the normal distribution.')\n",
    "        print('\\nIf the returned statistic is larger than these critical values then for the corresponding significance level,')\n",
    "        print(    'the null hypothesis that the data come from the chosen distribution can be rejected. \\n')\n",
    "        print('Source: https://variation.com/wp-content/distribution_analyzer_help/hs140.htm')\n",
    "        print('\\n')\n",
    "    \n",
    "        print(\"\\nShapiro's test\\n\")\n",
    "        print('The Shapiro test is based on the correlation between the data and the corresponding normal scores.')\n",
    "        print('As the dataset being evaluated gets larger, the Shapiro-Wilk test tends to be slightly more ')\n",
    "        print(    'sensitive to data in the tails which leads to a greater probability of rejecting the null hypothesis \\n')\n",
    "        print('Source: https://influentialpoints.com/Training/kolmogorov-smirnov_test-principles-properties-assumptions.htm\\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        print('Summary table for Data set')\n",
    "        display(table[['features', 'count', 'mean', 'std', 'min', '25%','50%', '75%', 'max', 'variance', 'IQR', 'range', 'skewness', 'kurtosis', 'mode', 'Kolmogorov Statistics', 'Kolmogorov P-value', 'Anderson-Darling Statistics', 'Anderson-Darling Critical Value', 'Shapiro Statistics', 'Shapiro P-value', 'Defined Significance Level']])\n",
    "        \n",
    "        \n",
    "    sumr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be947507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_categorical(df,# data frame, if variable inform as df[[var]]\n",
    "                    size=None,# size of the plot to be create: \"L\" large,otherwise Small\n",
    "                    label=None, #if the bar plot should have labels and ticks\n",
    "                    ref=None, # ref add a y-axis reference line inform value\n",
    "                   ):\n",
    "    '''This function returns summarization and visual for univariate analysis of categorical features of a dataset'''\n",
    "    print('############### Univariate analysis - Categorical ###############')\n",
    "# Setting canvas  \n",
    "    sns.color_palette(\"dark\")    #, as_cmap=True\n",
    "    sns.set_context(\"talk\", font_scale=0.8) #paper, notebook, talk, poster\n",
    "\n",
    "    #extracting only non-numerical features\n",
    "    df = df.select_dtypes(exclude=np.number) \n",
    "    for i in df.columns:\n",
    "     #   if type(df.loc[0, i]) == str: #checking type of features\n",
    "# Summarization\n",
    "            d1 = df[i].value_counts(dropna=False)\n",
    "            d2 = round(df[i].value_counts(normalize=True,dropna=False)*100, 2)\n",
    "            d3 = pd.concat([d1, d2], axis=1)\n",
    "            d3.columns = ['Count', 'Percentage']\n",
    "            print('\\nThis is univariate analysis for', \"'\", i, \"'\", '\\n', d3)\n",
    "\n",
    " # Visualization\n",
    "            d1 = pd.DataFrame(d1).T\n",
    "            d1 = pd.melt(d1,var_name=str(i),value_vars=d1) \n",
    "            d1[i]=d1[i].fillna('Missing') \n",
    "            fig, ax = plt.subplots()\n",
    "            if size == 'L':\n",
    "                fig.set_size_inches(22, 10)\n",
    "            else:\n",
    "                fig.set_size_inches(8, 8)\n",
    "            plot1=sns.barplot(y=d1['value'],x=d1[i], data=d1)#,color='darkblue'\n",
    "            if label == 'Y':\n",
    "                for p in ax.patches:\n",
    "                    ax.annotate(int(p.get_height()), \n",
    "                    (p.get_x()+p.get_width()/2+.3, p.get_height()),        \n",
    "                    ha='center',va='center',xytext=(2, 10),textcoords='offset points', rotation=45)\n",
    "            else:\n",
    "                plt.tick_params(\n",
    "                    axis='x',          # changes apply to the x-axis\n",
    "                    which='both',      # both major and minor ticks are affected,'major', 'minor', 'both'\n",
    "                    bottom=False,      # ticks along the bottom edge are off\n",
    "                    top=False,         # ticks along the top edge are off\n",
    "                    labelbottom=False) # labels along the bottom edge are off\n",
    "            plt.xticks(rotation=45, ha='right') \n",
    "            if ref != None:\n",
    "                plot1.axhline(ref,c='red',label='threshold='+str(ref),linestyle='--')\n",
    "                plt.legend(loc=1)\n",
    "            ax.set_title('Univariate plot')\n",
    "            plt.show()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208debcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_categorical_cat(df_var_pred, df_var_target,size):\n",
    "    '''This function returns summarization, visual,and test of independency for bivariate analysis of categorical\n",
    "    x categorical features. df[var_pred], df[var_target]'''\n",
    "# Setting canvas  \n",
    "    sns.color_palette(\"dark\")    #, as_cmap=True\n",
    "    sns.set_context(\"talk\", font_scale=0.8) #paper, notebook, talk, poster\n",
    "    \n",
    "    \n",
    "    contigency_table = pd.crosstab(df_var_pred,df_var_target)\n",
    "    \n",
    "    print('This is contingency table for:\\n\\n',contigency_table,'\\n')\n",
    "\n",
    "    alpha = 0.05\n",
    "    chi_2, p_val, dof, exp_val = chi2_contingency(contigency_table)\n",
    "    if size == 'L':\n",
    "        contigency_table.plot(kind='bar',figsize=(15,6))\n",
    "    else:\n",
    "        contigency_table.plot(kind='bar',figsize=(8,3))\n",
    "    plt.title('Bivariate Analysis: ' + str(df_var_target.name) +' vs ' + str(df_var_pred.name))\n",
    "#     plt.ylim(0,3500)\n",
    "    plt.legend(ncol=2,fontsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nChi-square\\'s Null hypothesis: There\\'s no correlation between variables.')\n",
    "    print(\"\\nChi-square's results:\")\n",
    "    if p_val > alpha:\n",
    "        print('DOF: ',dof,'\\np-value= ',p_val,\"\\n Fail to reject Null Hypothesis. There's no correlation between variables at 5% significance level.\")\n",
    "    else:\n",
    "        print('DOF: ',dof,'\\np-value= ',p_val,\"\\n\\nReject Null Hypothesis. There's some correlation between variables at 5% significance level.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3429214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####In development\n",
    "\n",
    "\n",
    "\n",
    "def bi_numerical_num(df):\n",
    "    '''This function returns summarization, visual,and test of independency for bivariate analysis of numerical x numerical features. '''\n",
    "\n",
    "    def hoeffding(*arg):\n",
    "    #Source: https://github.com/PaulVanDev/HoeffdingD     \n",
    "    \n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from scipy.stats import rankdata\n",
    "        from scipy.signal import decimate\n",
    "        import math\n",
    "        import time\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.preprocessing import KBinsDiscretizer\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "  \n",
    "        if(len(arg)==1):\n",
    "          if isinstance(arg[0], pd.DataFrame):\n",
    "            if(arg[0].shape[0]>1):\n",
    "              return arg[0].apply(lambda x: arg[0].apply(lambda y: hoeffding(x.values, y.values)))\n",
    "        else:\n",
    "          if(len(arg)==2):\n",
    "            if type(arg[0]) is not np.ndarray:\n",
    "              if (len(arg[0].shape)>1):\n",
    "                return print(\"ERROR inputs : hoeffding(df >2col) or hoeffding(numpy.array -1d- ,numpy.array -1d-)\")\n",
    "            if type(arg[1]) is np.ndarray:\n",
    "              if (len(arg[0].shape)>1):\n",
    "                return print(\"ERROR inputs : hoeffding(df >2col) or hoeffding(numpy.array -1d- ,numpy.array -1d-)\")\n",
    "        \n",
    "            xin=arg[0]\n",
    "            yin=arg[1]\n",
    "            #crop data to the smallest array, length have to be equal\n",
    "            if len(xin)<len(yin):\n",
    "              yin=yin[:len(xin)]\n",
    "            if len(xin)>len(yin):\n",
    "              xin=xin[:len(yin)]\n",
    "\n",
    "            # dropna\n",
    "            x = xin[~(np.isnan(xin) | np.isnan(yin))]\n",
    "            y = yin[~(np.isnan(xin) | np.isnan(yin))]\n",
    "\n",
    "            # undersampling if length too long\n",
    "            lenx=len(x)\n",
    "            if lenx>99999:\n",
    "                factor=math.ceil(lenx/100000)\n",
    "                x=x[::factor]\n",
    "                y=y[::factor]\n",
    "\n",
    "            # bining if too much \"definition\"\n",
    "            if len(np.unique(x))>50:\n",
    "                est = KBinsDiscretizer(n_bins=50, encode='ordinal', strategy='quantile') #faster strategy='quantile' but less accurate\n",
    "                est.fit(x.reshape(-1, 1))  \n",
    "                Rtemp = est.transform(x.reshape(-1, 1))\n",
    "                R=rankdata(Rtemp)\n",
    "            else:\n",
    "                R=rankdata(x)\n",
    "            if len(np.unique(y))>50:\n",
    "                est1 = KBinsDiscretizer(n_bins=50, encode='ordinal', strategy='quantile') #faster strategy='quantile' but less accurate\n",
    "                est1.fit(y.reshape(-1, 1))  \n",
    "                Stemp = est1.transform(y.reshape(-1, 1))\n",
    "                S=rankdata(Stemp)\n",
    "            else:\n",
    "                S=rankdata(y)      \n",
    "\n",
    "            # core processing\n",
    "            N=x.shape\n",
    "            dico={(np.nan,np.nan):np.nan}\n",
    "            dicoRin={np.nan:np.nan}\n",
    "            dicoSin={np.nan:np.nan}\n",
    "            dicoRless={np.nan:np.nan}\n",
    "            dicoSless={np.nan:np.nan}\n",
    "            Q=np.ones(N[0])\n",
    "\n",
    "            i=0;\n",
    "            for r,s in np.nditer([R,S]):\n",
    "                r=float(r)\n",
    "                s=float(s)\n",
    "                if (r,s) in dico.keys():\n",
    "                    Q[i]=dico[(r,s)]\n",
    "                else:\n",
    "                  if r in dicoRin.keys():\n",
    "                      isinR=dicoRin[r]\n",
    "                      lessR=dicoRless[r]\n",
    "                  else:\n",
    "                      isinR=np.isin(R,r)\n",
    "                      dicoRin[r]=isinR\n",
    "                      lessR=np.less(R,r)\n",
    "                      dicoRless[r]=lessR\n",
    "\n",
    "                  if s in dicoSin.keys():\n",
    "                      isinS=dicoSin[s]\n",
    "                      lessS=dicoSless[s]\n",
    "                  else:\n",
    "                      isinS=np.isin(S,s)\n",
    "                      dicoSin[s]=isinS\n",
    "                      lessS=np.less(S,s)\n",
    "                      dicoSless[s]=lessS\n",
    "\n",
    "\n",
    "                  Q[i] = Q[i] + np.count_nonzero(lessR & lessS) \\\n",
    "                        + 1/4 * (np.count_nonzero(isinR & isinS)-1) \\\n",
    "                        + 1/2 * (np.count_nonzero(isinR & lessS)) \\\n",
    "                         + 1/2 * (np.count_nonzero(lessR & isinS)) \n",
    "                  dico[(r,s)]=Q[i]\n",
    "                i+=1\n",
    "\n",
    "            D1 = np.sum( np.multiply((Q-1),(Q-2)) );\n",
    "            D2 = np.sum( np.multiply(np.multiply((R-1),(R-2)),np.multiply((S-1),(S-2)) ) );\n",
    "            D3 = np.sum( np.multiply(np.multiply((R-2),(S-2)),(Q-1)) );\n",
    "\n",
    "            D = 30*((N[0]-2)*(N[0]-3)*D1 + D2 - 2*(N[0]-2)*D3) / (N[0]*(N[0]-1)*(N[0]-2)*(N[0]-3)*(N[0]-4));\n",
    "\n",
    "            return D\n",
    "          return print(\"ERROR inputs : hoeffding(df >2col) or hoeffding(numpy.array -1d- ,numpy.array -1d-)\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# Filter warnings\n",
    "    warnings.filterwarnings('ignore',category=DeprecationWarning)#'always'    \n",
    "    \n",
    "    print('############### Bivariate analysis - Numerical x Numerical ###############')\n",
    "    \n",
    "    # Printing Assumptions/What tests for:\n",
    "    \n",
    "    #Pearson\n",
    "    print('\\nPearson Correlation (how strong the correlation is?):\\n Null hypothesis: there’s no association between variables.')\n",
    "    print('\\t1.Normal distribution for both variables for pearson;')\n",
    "    print('\\t2.homoscedasticity assumes that data is equally distributed about the regression line.')\n",
    "    print('\\t3.Assess Linear relationship')\n",
    "    \n",
    "    #Spearman      \n",
    "    print('\\nSpearman rank Correlation (how strong the correlation is?):\\n Null hypothesis: there’s no association between variables.')\n",
    "    print('\\t1.Data must be at least ordinal;')\n",
    "    print('\\t2.scores on one variable must be monotonically related to the other variable')     \n",
    "    print(\"\\t3.It's is appropriate for both continuous and discrete ordinal variables.\")\n",
    "    print(\"\\t4.Assess monotonic relationships.\")\n",
    "    \n",
    "    #Hoeffding      \n",
    "    print(\"\\nHoeffding's independence test(are the variables dependent?):\\n Null hypothesis: there’s no association between variables.')\")\n",
    "    print('\\t1.continuous distribution functions;')\n",
    "    print('\\t2. [...]the D statistic values are between –0.5 and 1, with 1 indicating complete dependence')     \n",
    "    print(\"\\t3. it has more power to detect non-monotonic dependency structures compared to other more common measures (Pearson, Kendall, Spearman)\")\n",
    "    print(\"\\t4.Assess non-monotonic non-linear relationships.\")\n",
    "    print('Source: https://github.com/PaulVanDev/HoeffdingD')\n",
    "    print('\\thttps://blogs.sas.com/content/iml/2021/04/28/compute-hoeffding-d-statistic.html')\n",
    "          \n",
    "\n",
    "# Verifying datatypes extract only numerics\n",
    "#numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    df = df.select_dtypes(include=np.number)\n",
    "    \n",
    "# # correlation test (summary)\n",
    "    #print('\\nThis is Pearson\\'s correlation:')\n",
    "    #display(df.corr(method='pearson'))\n",
    "    #print('\\nThis is Spearman\\'s correlation:')\n",
    "    #display(df.corr( method='spearman'))\n",
    "    #print('\\nThis is Spearman\\'s correlation:')\n",
    "    #display(hoeffding(df))\n",
    "    \n",
    "# Visualization\n",
    "    #scatter plot\n",
    "    g= sns.PairGrid(df, corner=True,despine=True)      \n",
    "    g.map_lower(sns.scatterplot, s=12)  \n",
    "    g.map_lower(sns.lineplot, color='r')\n",
    "    g.map_diag(sns.histplot)\n",
    "    g.fig.suptitle('Numeric Features - Scatterplots and Histograms')\n",
    "    plt.show()      \n",
    "     \n",
    "          \n",
    "    #Pearson\n",
    "    mask = np.zeros_like(df.corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    fig, ax = plt.subplots(figsize=(8,8)) \n",
    "    sns.set_palette(\"dark\") # colorblind   pastel\n",
    "    sns.set(style=\"whitegrid\") #whitegrid ticks\n",
    "    sns.heatmap(round(df.corr(method='pearson'),2),annot=True, annot_kws={\"fontsize\":11},linewidths=0.01,mask=mask)\n",
    "    plt.suptitle('Pearson correlation')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #Spearman\n",
    "    mask = np.zeros_like(df.corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    fig, ax = plt.subplots(figsize=(8,8)) \n",
    "    sns.set_palette(\"dark\") # colorblind   pastel\n",
    "    sns.set(style=\"whitegrid\") #whitegrid ticks\n",
    "    sns.heatmap(round(df.corr(method='spearman'),2),annot=True, annot_kws={\"fontsize\":11},linewidths=0.01,mask=mask)\n",
    "    plt.title('Spearman correlation')\n",
    "    plt.show()\n",
    "    \n",
    "    #Hoeffding\n",
    "    mask = np.zeros_like(df.corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    fig, ax = plt.subplots(figsize=(8,8)) \n",
    "    sns.set_palette(\"dark\") # colorblind   pastel\n",
    "    sns.set(style=\"whitegrid\") #whitegrid ticks\n",
    "    sns.heatmap(round(hoeffding(df),2),annot=True, annot_kws={\"fontsize\":11},linewidths=0.01,mask=mask)\n",
    "    plt.title(\"Hoeffding 's independence test\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc2fb2",
   "metadata": {},
   "source": [
    "#### Setting cell size for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272aa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<style>div.output_scroll { height: 99em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7871ccc",
   "metadata": {},
   "source": [
    "### Setting warnings OFF/ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('always')#'ignore'\n",
    "\n",
    "#Set theme for plots\n",
    "sns.set_theme(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb4bf0",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Directory\n",
    "os.chdir(r'D:\\Cursos\\Repositories\\Heart_Failure_Prediction\\Data\\Raw Data\\archive')\n",
    "\n",
    "df = pd.read_csv('heart_failure_clinical_records_dataset.csv',dtype={'anaemia':str, \n",
    "                                                                     'diabetes':str, \n",
    "                                                                     'high_blood_pressure':str, \n",
    "                                                                     'sex':str,\n",
    "                                                                     'smoking':str, \n",
    "                                                                     'DEATH_EVENT':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6878626",
   "metadata": {},
   "source": [
    "First look at dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of dataset: ',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77655e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features types:\\n ',df.dtypes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab36fc",
   "metadata": {},
   "source": [
    "Checking for missing values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03188c75",
   "metadata": {},
   "source": [
    "## Splitting Data set\n",
    "\n",
    "Setting target and dividing training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "target='DEATH_EVENT'\n",
    "\n",
    "# Creating copy of df for exploration data analysis:\n",
    "df_exp = df.copy()\n",
    "\n",
    "X = df_exp.drop([target],axis=1)\n",
    "y = df_exp[target]\n",
    "\n",
    "# Dividing into training and test datasets:\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,random_state=8,test_size=0.3)\n",
    "\n",
    "# Merging X and y to perform exploratory data analysis\n",
    "\n",
    "df_exp = pd.concat([X_train, y_train], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeef780",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis will be performed only on train data to get the most unbiased estimate\n",
    "\n",
    "### Univariate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef9ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chck_miss(df_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857f820",
   "metadata": {},
   "source": [
    "There are no missing values in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f760ef1",
   "metadata": {},
   "source": [
    "### Checking categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_level(df_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3e08e",
   "metadata": {},
   "source": [
    "#### Recoding values to original meaning to make interpretation meaningful:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dee708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp['anaemia'] = df_exp['anaemia'].map({'0':'No','1':'Yes'})\n",
    "df_exp['diabetes'] = df_exp['diabetes'].map({'0':'No','1':'Yes'})\n",
    "df_exp['high_blood_pressure'] = df_exp['high_blood_pressure'].map({'0':'No','1':'Yes'})\n",
    "df_exp['sex'] = df_exp['sex'].map({'0':'Female','1':'Male'})\n",
    "df_exp['smoking'] = df_exp['smoking'].map({'0':'No','1':'Yes'})\n",
    "df_exp['DEATH_EVENT'] = df_exp['DEATH_EVENT'].map({'0':'Censored','1':'Died'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb320e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(df_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7412ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_categorical(df_exp, label='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4a11b",
   "metadata": {},
   "source": [
    "Most features are unbalanced.\n",
    "\n",
    "Target feature 'DEATH_EVENT' have a high inbalance with most values being Censored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa40e2",
   "metadata": {},
   "source": [
    "### Checking numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf14bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_numerical(df_exp,visual='Y',test='A',alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24655efe",
   "metadata": {},
   "source": [
    "Age looks normal shaped. Ejection fraction has a couple of outliers. Platelets, serum creatinine phosphokinase and serum creatinine have a great number of outliers observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24940f",
   "metadata": {},
   "source": [
    "# Development\n",
    "## Next (immediate) phase:  \n",
    "\n",
    "    Bivariate anaysis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557eb6f",
   "metadata": {},
   "source": [
    "### Bivariate analysis\n",
    "\n",
    "#### Checking multicolilinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f1dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping target\n",
    "df_exp.drop('DEATH_EVENT',axis=1,inplace=True)\n",
    "df_exp.columns\n",
    "\n",
    "# Separting features by type: categorical and numerical\n",
    "numerical_columns = df_exp.select_dtypes(include=np.number) \n",
    "categorical_columns = df_exp.select_dtypes(exclude=np.number) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b576c",
   "metadata": {},
   "source": [
    "### Continuous x Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_numerical_num(df_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d3bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6caf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
